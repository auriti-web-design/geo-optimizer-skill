#!/usr/bin/env python3
"""
Schema Injector ‚Äî Adds JSON-LD schema to HTML pages and Astro components
Generative Engine Optimization (GEO) Toolkit

.. deprecated:: 2.0.0
    Use ``geo schema`` CLI instead. This script will be removed in v3.0.

Author: Juan Camilo Auriti (juancamilo.auriti@gmail.com)

Usage:
    geo schema --file index.html --analyze
"""

import warnings

warnings.warn(
    "scripts/schema_injector.py is deprecated. Use 'geo schema' CLI instead. "
    "This script will be removed in v3.0.",
    DeprecationWarning,
    stacklevel=1,
)

import argparse
import json
import re
import shutil
import sys
from typing import Dict, List

SCHEMA_TEMPLATES = {
    "website": {
        "@context": "https://schema.org",
        "@type": "WebSite",
        "name": "{{name}}",
        "url": "{{url}}",
        "description": "{{description}}",
        "potentialAction": {
            "@type": "SearchAction",
            "target": {"@type": "EntryPoint", "urlTemplate": "{{url}}/search?q={search_term_string}"},
            "query-input": "required name=search_term_string",
        },
    },
    "webapp": {
        "@context": "https://schema.org",
        "@type": "WebApplication",
        "name": "{{name}}",
        "url": "{{url}}",
        "description": "{{description}}",
        "applicationCategory": "UtilityApplication",
        "operatingSystem": "Web",
        "browserRequirements": "Requires JavaScript",
        "offers": {"@type": "Offer", "price": "0", "priceCurrency": "USD"},
        "author": {"@type": "Organization", "name": "{{author}}"},
    },
    "faq": {"@context": "https://schema.org", "@type": "FAQPage", "mainEntity": []},
    "article": {
        "@context": "https://schema.org",
        "@type": "Article",
        "headline": "{{title}}",
        "description": "{{description}}",
        "url": "{{url}}",
        "datePublished": "{{date_published}}",
        "dateModified": "{{date_modified}}",
        "author": {"@type": "Person", "name": "{{author}}"},
        "publisher": {
            "@type": "Organization",
            "name": "{{publisher}}",
            "logo": {"@type": "ImageObject", "url": "{{logo_url}}"},
        },
    },
    "organization": {
        "@context": "https://schema.org",
        "@type": "Organization",
        "name": "{{name}}",
        "url": "{{url}}",
        "description": "{{description}}",
        "logo": "{{logo_url}}",
        "sameAs": [],
    },
    "breadcrumb": {
        "@context": "https://schema.org",
        "@type": "BreadcrumbList",
        "itemListElement": [{"@type": "ListItem", "position": 1, "name": "Home", "item": "{{url}}"}],
    },
}


ASTRO_TEMPLATE = """\
---
// src/layouts/BaseLayout.astro ‚Äî GEO-optimized layout
// Generated by GEO Optimizer (github.com/auriti-labs/geo-optimizer-skill)
// siteUrl and siteName are set below ‚Äî update if needed.
interface Props {
  title: string;
  description: string;
  url?: string;
  isCalculator?: boolean;
  faqItems?: Array<{ question: string; answer: string }>;
}

const {
  title,
  description,
  url = Astro.url.href,
  isCalculator = false,
  faqItems = [],
} = Astro.props;

const siteUrl = "SITE_URL";
const siteName = "SITE_NAME";

const websiteSchema = {
  "@context": "https://schema.org",
  "@type": "WebSite",
  "name": siteName,
  "url": siteUrl,
  "description": description,
};

const webAppSchema = isCalculator ? {
  "@context": "https://schema.org",
  "@type": "WebApplication",
  "name": title,
  "url": url,
  "description": description,
  "applicationCategory": "UtilityApplication",
  "operatingSystem": "Web",
  "offers": { "@type": "Offer", "price": "0", "priceCurrency": "USD" }
} : null;

const faqSchema = faqItems.length > 0 ? {
  "@context": "https://schema.org",
  "@type": "FAQPage",
  "mainEntity": faqItems.map(item => ({
    "@type": "Question",
    "name": item.question,
    "acceptedAnswer": { "@type": "Answer", "text": item.answer }
  }))
} : null;
---

<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>{title} | {siteName}</title>
  <meta name="description" content={description} />
  <link rel="canonical" href={url} />

  <!-- Open Graph -->
  <meta property="og:title" content={title} />
  <meta property="og:description" content={description} />
  <meta property="og:url" content={url} />
  <meta property="og:type" content="website" />

  <!-- GEO: Schema JSON-LD -->
  <script type="application/ld+json" set:html={JSON.stringify(websiteSchema)} />
  {webAppSchema && <script type="application/ld+json" set:html={JSON.stringify(webAppSchema)} />}
  {faqSchema && <script type="application/ld+json" set:html={JSON.stringify(faqSchema)} />}
</head>
"""


def fill_template(template: dict, values: dict) -> dict:
    """Replace placeholders in the template with the provided values."""
    template_str = json.dumps(template)
    for key, value in values.items():
        template_str = template_str.replace(f"{{{{{key}}}}}", str(value) if value else "")
    return json.loads(template_str)


def schema_to_html_tag(schema_dict: dict) -> str:
    """Convert a schema dict to an HTML script tag."""
    json_str = json.dumps(schema_dict, indent=2, ensure_ascii=False)
    return f'<script type="application/ld+json">\n{json_str}\n</script>'


def extract_faq_from_html(soup) -> List[Dict[str, str]]:
    """
    Auto-extract FAQ items from HTML.
    Looks for common patterns:
    - <dt>question</dt><dd>answer</dd>
    - <div class="faq-item"><h3>Q</h3><p>A</p></div>
    - <details><summary>Q</summary>A</details>
    """
    faqs = []

    # Pattern 1: <dt> and <dd>
    dts = soup.find_all("dt")
    for dt in dts:
        dd = dt.find_next_sibling("dd")
        if dd:
            question = dt.get_text(strip=True)
            answer = dd.get_text(strip=True)
            if question and answer and len(question) > 5 and len(answer) > 10:
                faqs.append({"question": question, "answer": answer})

    # Pattern 2: <details> / <summary>
    details = soup.find_all("details")
    for detail in details:
        summary = detail.find("summary")
        if summary:
            question = summary.get_text(strip=True)
            # Answer is everything except the summary
            summary.extract()
            answer = detail.get_text(strip=True)
            if question and answer and len(question) > 5 and len(answer) > 10:
                faqs.append({"question": question, "answer": answer})

    # Pattern 3: Common FAQ class patterns
    faq_containers = soup.find_all(class_=re.compile(r"faq|question|qa", re.I))
    for container in faq_containers:
        # Try to find question (h3, h4, strong, or element with "question" class)
        q_elem = container.find(["h3", "h4", "strong"]) or container.find(class_=re.compile(r"question", re.I))
        if q_elem:
            question = q_elem.get_text(strip=True)
            # Answer is the rest
            q_elem.extract()
            answer = container.get_text(strip=True)
            if question and answer and len(question) > 5 and len(answer) > 10:
                faqs.append({"question": question, "answer": answer})

    return faqs


def analyze_html_file(file_path: str, verbose: bool = False) -> dict:
    """Analyze an HTML file and return found/missing schemas + extracted data."""
    try:
        from bs4 import BeautifulSoup
    except ImportError:
        print("‚ùå beautifulsoup4 required: pip install beautifulsoup4")
        sys.exit(1)

    with open(file_path, "r", encoding="utf-8") as f:
        content = f.read()

    soup = BeautifulSoup(content, "html.parser")

    # Extract all JSON-LD scripts
    found_schemas = []
    scripts = soup.find_all("script", type="application/ld+json")

    for idx, script in enumerate(scripts):
        try:
            # Handle both string content and NavigableString
            script_content = script.string
            if script_content:
                data = json.loads(script_content.strip())

                # Handle both single schema and array of schemas
                if isinstance(data, list):
                    for item in data:
                        schema_type = item.get("@type", "Unknown")
                        found_schemas.append({"type": schema_type, "data": item, "index": idx})
                else:
                    schema_type = data.get("@type", "Unknown")
                    found_schemas.append({"type": schema_type, "data": data, "index": idx})
        except json.JSONDecodeError as e:
            if verbose:
                print(f"‚ö†Ô∏è  Invalid JSON in script tag {idx}: {e}")
        except Exception as e:
            if verbose:
                print(f"‚ö†Ô∏è  Error parsing script tag {idx}: {e}")

    # Determine what's missing
    found_types = [s["type"] for s in found_schemas]
    missing = []

    if "WebSite" not in found_types:
        missing.append("website")
    if "WebApplication" not in found_types:
        missing.append("webapp")
    if "FAQPage" not in found_types:
        missing.append("faq")

    # Extract FAQ if FAQPage is missing
    extracted_faqs = []
    if "FAQPage" not in found_types:
        extracted_faqs = extract_faq_from_html(soup)

    # Check for duplicates
    duplicates = {}
    for schema_type in set(found_types):
        count = found_types.count(schema_type)
        if count > 1:
            duplicates[schema_type] = count

    return {
        "found_schemas": found_schemas,
        "found_types": found_types,
        "missing": missing,
        "extracted_faqs": extracted_faqs,
        "duplicates": duplicates,
        "has_head": bool(soup.find("head")),
        "total_scripts": len(scripts),
    }


def generate_faq_schema(faq_items: List[Dict[str, str]]) -> dict:
    """Generate FAQPage schema from FAQ items."""
    schema = SCHEMA_TEMPLATES["faq"].copy()
    schema["mainEntity"] = [
        {"@type": "Question", "name": item["question"], "acceptedAnswer": {"@type": "Answer", "text": item["answer"]}}
        for item in faq_items
    ]
    return schema


def inject_schema_into_html(file_path: str, schema_dict: dict, backup: bool = True, validate: bool = True) -> bool:
    """
    Inject a schema tag into an HTML file (before </head>).

    Args:
        file_path (str): Path to HTML file
        schema_dict (dict): Schema dictionary to inject
        backup (bool): Create .bak backup before modifying
        validate (bool): Validate schema before injection (recommended)

    Returns:
        bool: True if successful, False otherwise
    """
    try:
        from bs4 import BeautifulSoup
    except ImportError:
        print("‚ùå beautifulsoup4 required: pip install beautifulsoup4")
        return False

    # Validate schema before injection (Fix #7)
    if validate:
        from schema_validator import validate_jsonld

        # Infer schema type from @type field for stricter validation
        schema_type_field = schema_dict.get("@type")
        if isinstance(schema_type_field, list):
            schema_type = schema_type_field[0].lower() if schema_type_field else None
        elif isinstance(schema_type_field, str):
            schema_type = schema_type_field.lower()
        else:
            schema_type = None

        is_valid, error_msg = validate_jsonld(schema_dict, schema_type, strict=False)
        if not is_valid:
            print(f"‚ö†Ô∏è  Schema validation failed: {error_msg}")
            print("   Use --no-validate to inject anyway (not recommended)")
            return False
        print("‚úÖ Schema validation passed")

    # Backup (copy, not move ‚Äî preserves original if injection fails)
    if backup:
        backup_path = f"{file_path}.bak"
        shutil.copy2(file_path, backup_path)
        print(f"üìÅ Backup created: {backup_path}")

    with open(file_path, "r", encoding="utf-8") as f:
        content = f.read()

    soup = BeautifulSoup(content, "html.parser")
    head = soup.find("head")

    if not head:
        print("‚ùå No <head> tag found in HTML")
        return False

    # Create new schema tag
    schema_tag = soup.new_tag("script", type="application/ld+json")
    schema_tag.string = "\n" + json.dumps(schema_dict, indent=2, ensure_ascii=False) + "\n"

    # Insert before </head>
    head.append(schema_tag)

    # Write back
    with open(file_path, "w", encoding="utf-8") as f:
        f.write(str(soup))

    return True


def print_analysis(analysis: dict, verbose: bool = False):
    """Pretty-print analysis results."""
    print(f"\n{'=' * 60}")
    print("  SCHEMA ANALYSIS")
    print(f"{'=' * 60}\n")

    if analysis["found_schemas"]:
        print(f"‚úÖ Found {len(analysis['found_schemas'])} schema(s):\n")
        for idx, schema in enumerate(analysis["found_schemas"], 1):
            schema_type = schema["type"]
            data = schema["data"]

            print(f"   {idx}. {schema_type}")

            # Show key properties
            if schema_type == "WebSite":
                print(f"      url: {data.get('url', 'N/A')}")
                print(f"      name: {data.get('name', 'N/A')}")
            elif schema_type == "WebApplication":
                print(f"      url: {data.get('url', 'N/A')}")
                print(f"      name: {data.get('name', 'N/A')}")
            elif schema_type == "FAQPage":
                faq_count = len(data.get("mainEntity", []))
                print(f"      questions: {faq_count}")
            elif schema_type == "Organization":
                print(f"      name: {data.get('name', 'N/A')}")
            elif schema_type == "BreadcrumbList":
                items = len(data.get("itemListElement", []))
                print(f"      items: {items}")

            if verbose:
                print("\n      Full schema:")
                print(f"      {json.dumps(data, indent=6, ensure_ascii=False)}\n")
            print()
    else:
        print("‚ö†Ô∏è  No JSON-LD schemas found\n")

    # Duplicates warning
    if analysis["duplicates"]:
        print("‚ö†Ô∏è  DUPLICATE SCHEMAS DETECTED:\n")
        for schema_type, count in analysis["duplicates"].items():
            print(f"   ‚Ä¢ {schema_type}: {count} instances (should be 1)")
        print()

    # Missing schemas
    if analysis["missing"]:
        print("üí° Suggested schemas to add:\n")
        for schema_type in analysis["missing"]:
            print(f"   ‚Ä¢ {schema_type.upper()}")
        print()

    # Extracted FAQs
    if analysis["extracted_faqs"]:
        print(f"üìã Auto-detected {len(analysis['extracted_faqs'])} FAQ items:\n")
        for idx, faq in enumerate(analysis["extracted_faqs"][:3], 1):  # Show first 3
            q = faq["question"][:60] + "..." if len(faq["question"]) > 60 else faq["question"]
            print(f"   {idx}. {q}")
        if len(analysis["extracted_faqs"]) > 3:
            print(f"   ... and {len(analysis['extracted_faqs']) - 3} more")
        print()
        print("   üí° Use --type faq --auto-extract --inject to add FAQPage schema")
        print()


def main():
    parser = argparse.ArgumentParser(
        description="Inject JSON-LD schema into HTML pages or generate Astro snippets",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Analyze HTML file and show existing schemas
  ./geo scripts/schema_injector.py --file index.html --analyze

  # Analyze with verbose output (shows full schema JSON)
  ./geo scripts/schema_injector.py --file index.html --analyze --verbose

  # Inject WebSite schema
  ./geo scripts/schema_injector.py --file index.html --type website --name "MySite" --url https://example.com --inject

  # Auto-extract FAQ and inject FAQPage schema
  ./geo scripts/schema_injector.py --file page.html --type faq --auto-extract --inject

  # Generate Astro BaseLayout snippet
  ./geo scripts/schema_injector.py --type website --name "MySite" --url https://example.com --astro
        """,
    )

    parser.add_argument("--file", help="HTML file to analyze/modify")
    parser.add_argument("--type", choices=list(SCHEMA_TEMPLATES.keys()), help="Type of schema to generate")
    parser.add_argument("--name", help="Site/application name")
    parser.add_argument("--url", help="Site URL")
    parser.add_argument("--description", help="Description")
    parser.add_argument("--author", help="Author")
    parser.add_argument("--logo-url", help="Logo URL")
    parser.add_argument("--faq-file", help="JSON file with FAQs [{question, answer}]")
    parser.add_argument("--auto-extract", action="store_true", help="Auto-extract FAQ from HTML")
    parser.add_argument("--astro", action="store_true", help="Generate Astro snippet")
    parser.add_argument("--inject", action="store_true", help="Inject directly into --file")
    parser.add_argument("--no-backup", action="store_true", help="Do not create backup before modifying")
    parser.add_argument(
        "--no-validate", action="store_true", help="Skip schema validation before injection (not recommended)"
    )
    parser.add_argument("--analyze", action="store_true", help="Only analyze the file, do not modify")
    parser.add_argument("--verbose", action="store_true", help="Show full schema JSON in analysis")

    args = parser.parse_args()

    # Mode 1: Analyze only
    if args.analyze:
        if not args.file:
            print("‚ùå --file required for --analyze")
            sys.exit(1)

        analysis = analyze_html_file(args.file, verbose=args.verbose)
        print_analysis(analysis, verbose=args.verbose)
        sys.exit(0)

    # Mode 2: Generate Astro snippet
    if args.astro:
        if not args.url or not args.name:
            print("‚ùå --url and --name required for --astro")
            sys.exit(1)

        snippet = ASTRO_TEMPLATE.replace("SITE_URL", args.url).replace("SITE_NAME", args.name)
        print(snippet)
        sys.exit(0)

    # Mode 3: Generate/inject schema
    if args.type:
        # Build schema
        if args.type == "faq":
            if args.auto_extract and args.file:
                # Auto-extract from HTML
                analysis = analyze_html_file(args.file)
                faq_items = analysis["extracted_faqs"]
                if not faq_items:
                    print("‚ùå No FAQ items found in HTML")
                    sys.exit(1)
                print(f"‚úÖ Extracted {len(faq_items)} FAQ items")
                schema = generate_faq_schema(faq_items)
            elif args.faq_file:
                # Load from JSON file
                with open(args.faq_file, "r") as f:
                    faq_items = json.load(f)
                schema = generate_faq_schema(faq_items)
            else:
                print("‚ùå --auto-extract or --faq-file required for FAQ schema")
                sys.exit(1)
        else:
            # Standard template
            values = {
                "name": args.name or "",
                "url": args.url or "",
                "description": args.description or "",
                "author": args.author or "",
                "logo_url": args.logo_url or "",
            }
            schema = fill_template(SCHEMA_TEMPLATES[args.type], values)

        # Output or inject
        if args.inject:
            if not args.file:
                print("‚ùå --file required for --inject")
                sys.exit(1)

            success = inject_schema_into_html(
                args.file, schema, backup=not args.no_backup, validate=not args.no_validate
            )
            if success:
                print(f"‚úÖ Schema injected into {args.file}")
            else:
                print("‚ùå Failed to inject schema")
                sys.exit(1)
        else:
            # Print schema JSON
            print(schema_to_html_tag(schema))
    else:
        parser.print_help()
        sys.exit(1)


if __name__ == "__main__":
    main()
